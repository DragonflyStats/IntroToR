\documentclass[a4paper,12pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{eurosym}
\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{fancyhdr}
%\usepackage{listings}
\usepackage{framed}
\usepackage{graphicx}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.00.0.2570}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{LastRevised=Wednesday, February 23, 2011 13:24:34}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{Language=American English}

\pagestyle{fancy}
\setmarginsrb{20mm}{0mm}{20mm}{25mm}{12mm}{11mm}{0mm}{11mm}
\lhead{Dublin \texttt{R}} \rhead{September 2013}
\chead{Introduction to \texttt{R}}
%\input{tcilatex}


% http://www.norusis.com/pdf/SPC_v13.pdf
\begin{document}

\tableofcontents

Part 1a : Exploratory Data Analysis and Basic Probability

Measures and Graphical Techniques (i.e. Boxplots)
Statistical Fallacies
Part 2 : Probability Distributions

Introduction to Probability
Probability Distributions
Advanced Probability
Probability Distributions (Gambler's Ruin)
Testing the assumption of normality
Part 2a: Inference Procedures

Part 2b: More on Inference Procedures

Grubb's Outlier Test
Kolomogorov Smirnov Test


Part 3 :  Industrial Statistics

Control Charts
Statistical Process Control
Process Capability Indices
Multivariate SPC
Part 4 : Statistical Modelling of Continuous Variables

Simple Linear Regression
Multiple Linear Regression
Testing Goodness of Fit  (AIC, Likelihood)
Testing Model Assumptions and Residual Analysis
Robust Regression Models
Part 5 :  Classification Problems

Binary Logistic Regression
Multinomial Logistics Regression
Ordinary Logistic Regression
Testing Model Assumptions and Diagnostics
Part 6 :  Modelling Count Variables

The Poisson Process
Poisson Regression
Negative Binomial Regression
Zero-Inflation Models
Truncated Models
Part 7 : Reliability Analysis (Survival Models)






R and EVT

R and EVT
Overview of Proposal
Simple R Programming
vectors
integer
Character
logical
numeric
Packages
Inference procedures
Regression models and bivariate data
Tinn-R
Packages
Using and Installing packages
Installing additional packages

Overview of Proposal
how to access R
using modules
setting up projects
statistics
designing a statistical analysis
practical element focusing carrying out basic statistics and extreme value analysis on an annual maximum series
 
 
head()
length()
rbind()
cbind()

x=c(x,16)

adding a value
deleting a value
x=x[-9]

commenting #######


basic R editor
 - new script
 - updating script
 - running script
 -

Introduction to R
 - What is R
 - History of R
Simple R Programming
 - Important R functions
  - help()
  - summary()
"summary" is a generic function used to produce result summaries of the results of various model fitting functions. 
The function invokes particular methods which depend on the class of the first argument. 
  - attach()
  - data()
   - Loads specified data sets, or list the available data sets. 
  - ?
  - data.entry
  - c()
- Combine Values into a Vector or List
  - assignment operator
  - Sys.time and Sys.Date returns the system's idea of the current date with and without time. 
  - q()


mode() - storage mode of a data object( data structure)

The expression as(object, value) is the way to coerce an object to a particular class. 

vectors

vector produces a vector of the given length and mode.
as.vector, a generic, attempts to coerce its argument into a vector of mode mode (the default is to coerce to whichever mode is most convenient).
is.vector returns TRUE if x is a vector of the specified mode having no attributes other than names. It returns FALSE otherwise.  
integer
Creates or tests for objects of type "integer".
integer(length = 0)
as.integer(x, ...)
is.integer(x)
 Character
Create or test for objects of type "character".
character(length = 0)
as.character(x, ...)
is.character(x)
logical
Create or test for objects of type "logical", and the basic logical constants.
logical(length = 0)
as.logical(x, ...)
is.logical(x)
numeric
Creates or coerces objects of type "numeric". 
numeric(length = 0)
as.numeric(x, ...)
is.numeric(x)



more data structures
 - lists
   - list() Functions to construct, coerce and check for both kinds of R lists. 
 - vectors
 - dataframes
 
Packages

 - What are pacakges
- extending the program
 - where are packages to be found
 - some notable packages
 - installing packages
 - loading packages
 - updating packages
 
Probability distributions
 - Continuous: Normal / Student's 't' / Chi-square distribution
 - discrete: Binomial / Poisson / geometric distribution
Inference procedures
 - t.test
Performs one and two sample t-tests on vectors of data. 
 - prop.test
	    prop.test can be used for testing the null that the proportions (probabilities of success) in several groups are the same, or that they equal certain given values.
 - Kolmogorov-Smirnov test      [ks.test()]
Performs one or two sample Kolmogorov-Smirnov tests. 
 - Anderson Darling test        [ad.test()]
 - Grubbs test for outliers     [Grubbs.test()]
 - Dixon test for outliers
 - correlation test [cor.test()]
 - Analysis of variance. [anova()]
 

Logical functions and coercion
 - as.vector
 - is.na
 - NA is a logical constant of length 1 which contains a missing value indicator.
 - Missing value/ not available. 
Regression models and bivariate data
 - simple linear regression 
 - multiple linear regression 
 - lm()
 - lm is used to fit linear models. It can be used to carry out regression, single stratum analysis of variance and analysis of covariance 

Control Loops
 - do 
 - if 

Writing Functions

Goodness of Fit statistics
 - AIC()



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Setting up project directories

It is helpful to organize your work in project directories.
Create a directory for your project.
Copy a workspace (a .Rdata file) to the directory
You can then start R by clicking on the .Rdata file's icon
All directory references in the R session will be relative to the project directory. For example, you can read a file 'data.csv' in the directory with


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
http://www.r-tutor.com/elementary-statistics/hypothesis-testing/two-tailed-test-population-proportion
 

Regression more
Chi-square Tests
Binomial coefficients
Basic Probability Distributions
Continuous Probability Distributions
The Normal distribution
other distributions
Central Limit Theorem

Regression more

http://www.ats.ucla.edu/stat/R/dae/rreg.htm

Grouping, loops and conditional execution
R is an expression language in the sense that its only command type is a function or expression which returns a result. Even an assignment is an expression whose result is the value assigned, and it may be used wherever any expression may be used; in particular multiple assignments are possible.

Commands may be grouped together in braces, {expr_1; ...; expr_m}, in which case the value of the group is the result of the last expression in the group evaluated. Since such a group is also an expression it may, for example, be itself included in parentheses and used a part of an even larger expression, and so on.  


[if Statement]

Have already encountered implicit looping when using the apply
family of functions.

Conditional execution: the if statement has the form 

if (condition){ # Brackets can be omitted if only one command
expr_1 # to be carried out.
}
else {
expr_2
}

The condition must evaluate to a logical value, i.e. TRUE or
FALSE. If the condition == TRUE, expr_1 is carried out, which
can consist of a single command or multiple commands. If the
condition == FALSE, expr_2 is carried out.

[if Statement]
Can also have longer if statements:
if (condition1){
expr_1
}
else if (condition2){
expr_2
}
...
else {
expr_n
}
If condition1 == TRUE, expr_1 is executed and the checking
stops. If condition1 == FALSE, moves on to condition2 and
checks if that condition is met. If condition2 == TRUE, expr_2
is executed and checking stops. If condition2 == FALSE, moves
on to the next condition and so on until all conditions have been
checked.
The nal else is executed if none of the previous conditions have
returned a value of TRUE.

[if Statement]
Usually the logical operators &&, ||, ==, !=, >, <, >=, <= are used
as the conditions in the if statement.
The following function gives a demonstration of the use of
if... else.
comparisons1 <- function(number)
{
# if ... else
if (number != 1)
{
cat(number,"is not one\n")
}
else
{
cat(number,"is one\n")
}
}
> comparisons1(1) > comparisons1(20)
1 is one 20 is not one

[if Statement]
The following demonstrates the use of
if ... else if ... else
comparisons2 <- function(number)
{
if (number == 0)
{
cat(number,"equals 0\n")
}
else if (number > 0)
{
cat(number,"is positive\n")
}
else
{
cat(number,"is negative\n")
}
}
> comparisons2(0) > comparisons2(-15) > comparisons2(1)
0 equals 0 -15 is negative 1 is positive

[if Statement]
This function demonstrates the use of && in the condition. This
means that both conditions must be met before a value of TRUE is
returned.
comparisons3 <- function(number)
{
if ( (number > 0) && (number < 10) )
{
cat(number,"is between 0 and 10\n")
}
}
> comparisons3(-1) > comparisons3(9) > comparisons3(10)
9 is between 0 and 10


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


 

Chi-square Tests
Hypothesis test for count data that use the Pearson Chi-square statistic are available in R.
These include the goodness-of-fit tests and those for contingency tables. Each of these are
performed by using the chisq.test() function. 

The basic syntax for this function is (see ?chisq.test for more information): 
 
http://www.r-tutor.com/elementary-statistics/probability-distributions

A probability distribution describes how the values of a random variable is distributed. For example, the collection of all possible outcomes of a sequence of coin tossing is known to follow the binomial distribution. Whereas the means of sufficiently large samples of a data population are known to resemble the normal distribution. Since the characteristics of these theoretical distributions are well understood, they can be used to make statistical inferences on the entire data population as a whole.
In the following tutorials, we demonstrate how to compute a few well-known probability distributions that occurs frequently in statistical study. We reference them quite often in other sections.

Binomial Distribution
Poisson Distribution
Continuous Uniform Distribution
Exponential Distribution
Normal Distribution
Chi-squared Distribution
Student t Distribution
F Distribution


Binomial coefficients
' n choose k'
nk =n!k! (n-k)!

RSS Higher Certificate Module 4 Linear model
(Royal Statistical Society Professional Exams)
Advanced Data Modelling

Use Simple Linear Regression for calibration and reverse prediction,
Apply and evaluate regression diagnostics with emphasis on leverage and influence points.
Explain the Matrix formulation of the linear model
Understand multiple regression, partial correlation, polynomial regression.
Apply Analysis of Variance : multiple comparisons, two-way ANOVA, interactions
Understand analysis of covariance.
Overview of Generalized Linear Models including nonlinear regression, logistic regression and log-linear models.
1. Simple Linear Regression :

calibration, reverse prediction, regression through the origin, analysis of residuals, regression diagnostics, leverage and influence.
2. Matrix formulation of the linear model :

Multiple regression, partial correlation, polynomial regression.

Regression

Simple linear regression. Least squares estimation.
Multiple linear regression – concepts, interpretation of computer output, inference for regression coefficients using estimates and estimated standard errors from computer output.
Analysis of variance for regression models.
Calculation and interpretation of the multiple correlation coefficient (coefficient of determination).
Simple cases of transforming to linearity.
Correlation

Product-moment correlation (Pearson).
Rank correlation – Spearman’s coefficient.
Calculation and interpretation.
Design of experiments

Reasons for experimentation, causality.
Principles of replication and randomisation, completely randomised design.
Analysis of variance

One-way analysis of variance.
Inference for means and for differences in means
One-way ANOVA, multiple comparisons,
Two-way ANOVA with  interactions,
Analysis of covariance.
Generalized Linear Models

Introduction to Generalized Linear Models including nonlinear regression, logistic regression and log-linear models.
%=============================================%
Basic Probability Distributions

We look at some of the basic operations associated with probability distributions. There are a large number of probability distributions available, but we only look at a few. If you would like to know what distributions are available you can do a search using the command help.search("distribution").

Here we give details about the commands associated with the normal distribution and briefly mention the commands for other distributions. The functions for different distributions are very similar where the differences are noted below.

For this chapter it is assumed that you know how to enter data which is covered in the first chapter.

The Normal Distribution
The t Distribution
The Binomial Distribution
The Chi-Squared Distribution
The Normal Distribution

There are four functions that can be used to generate the values associated with the normal distribution. You can get a full list of them and their options using the help command:

> help(Normal)
The first function we look at it dnorm. Given a set of values it returns the height of the probability distribution at each point. If you only give the points it assumes you want to use a mean of zero and standard deviation of one. 

There are options to use different values for the mean and standard deviation, though:

> dnorm(0)
[1] 0.3989423
> dnorm(0)*sqrt(2*pi)
[1] 1
> dnorm(0,mean=4)    
[1] 0.0001338302
> dnorm(0,mean=4,sd=10)
[1] 0.03682701
>v <- c(0,1,2)
> dnorm(v)
[1] 0.39894228 0.24197072 0.05399097
> x <- seq(-20,20,by=.1)
> y <- dnorm(x) 
> plot(x,y)
> y <- dnorm(x,mean=2.5,sd=0.1) 
> plot(x,y)
The second function we examine is pnorm. Given a number or a list it computes the probability that a normally distributed random number will be less than that number. 

This function also goes by the rather ominous title of the "Cumulative Distribution Function." It accepts the same options as dnorm:

> pnorm(0)
[1] 0.5
> pnorm(1)
[1] 0.8413447
> pnorm(0,mean=2)
[1] 0.02275013
> pnorm(0,mean=2,sd=3)
[1] 0.2524925
> v <- c(0,1,2)         
> pnorm(v)
[1] 0.5000000 0.8413447 0.9772499
> x <- seq(-20,20,by=.1)
> y <- pnorm(x) 
> plot(x,y)
> y <- pnorm(x,mean=3,sd=4) 
> plot(x,y)
The next function we look at is qnorm which is the inverse of pnorm. The idea behind qnorm is that you give it a probability, and it returns the number whose cumulative distribution matches the probability. For example, if you have a normally distributed random variable with mean zero and standard deviation one, then if you give the function a probability it returns the associated Z-score:

> qnorm(0.5)
[1] 0
> qnorm(0.5,mean=1)
[1] 1
> qnorm(0.5,mean=1,sd=2)
[1] 1
> qnorm(0.5,mean=2,sd=2)
[1] 2
> qnorm(0.5,mean=2,sd=4)
[1] 2
> qnorm(0.25,mean=2,sd=2)
[1] 0.6510205
> qnorm(0.333)
[1] -0.4316442
> qnorm(0.333,sd=3)
[1] -1.294933
> qnorm(0.75,mean=5,sd=2)
[1] 6.34898
> v = c(0.1,0.3,0.75)
> qnorm(v)
[1] -1.2815516 -0.5244005  0.6744898
> x <- seq(0,1,by=.05)
> y <- qnorm(x)
> plot(x,y)
> y <- qnorm(x,mean=3,sd=2)
> plot(x,y)
> y <- qnorm(x,mean=3,sd=0.1)
> plot(x,y)
The last function we examine is the rnorm function which can generate random numbers whose distribution is normal. The argument that you give it is the number of random numbers that you want, and it has optional arguments to specify the mean and standard deviation:

> rnorm(4)
[1]  1.2387271 -0.2323259 -1.2003081 -1.6718483
> rnorm(4,mean=3)
[1] 2.633080 3.617486 2.038861 2.601933
> rnorm(4,mean=3,sd=3)
[1] 4.580556 2.974903 4.756097 6.395894
> rnorm(4,mean=3,sd=3)
[1]  3.000852  3.714180 10.032021  3.295667
> y <- rnorm(200)
> hist(y)
> y <- rnorm(200,mean=-2)
> hist(y)
> y <- rnorm(200,mean=-2,sd=4)
> hist(y)
> qqnorm(y)
> qqline(y)
The t Distribution

There are four functions that can be used to generate the values associated with the t distribution. You can get a full list of them and their options using the help command:

> help(TDist)
These commands work just like the commands for the normal distribution. One difference is that the commands assume that the values are normalized to mean zero and standard deviation one, so you have to use a little algebra to use these functions in practice. The other difference is that you have to specify the number of degrees of freedom. The commands follow the same kind of naming convention, and the names of the commands are dt, pt, qt, and rt.

A few examples are given below to show how to use the different commands. First we have the distribution function, dt:

> x <- seq(-20,20,by=.5)
> y <- dt(x,df=10)
> plot(x,y)
> y <- dt(x,df=50)
> plot(x,y)
Next we have the cumulative probability distribution function:

> pt(-3,df=10)
[1] 0.006671828
> pt(3,df=10)
[1] 0.9933282
> 1-pt(3,df=10)
[1] 0.006671828
> pt(3,df=20)
[1] 0.996462
> x = c(-3,-4,-2,-1)
> pt((mean(x)-2)/sd(x),df=20)
[1] 0.001165548
> pt((mean(x)-2)/sd(x),df=40)
[1] 0.000603064

Next we have the inverse cumulative probability distribution function:

> qt(0.05,df=10)
[1] -1.812461
> qt(0.95,df=10)
[1] 1.812461
> qt(0.05,df=20)
[1] -1.724718
> qt(0.95,df=20)
[1] 1.724718
> v <- c(0.005,.025,.05)
> qt(v,df=253)
[1] -2.595401 -1.969385 -1.650899
> qt(v,df=25)
[1] -2.787436 -2.059539 -1.708141
> 
Finally random numbers can be generated according to the t distribution:

> rt(3,df=10)
[1] 0.9440930 2.1734365 0.6785262
> rt(3,df=20)
[1]  0.1043300 -1.4682198  0.0715013
> rt(3,df=20)
[1]  0.8023832 -0.4759780 -1.0546125
The Binomial Distribution

There are four functions that can be used to generate the values associated with the binomial distribution. You can get a full list of them and their options using the help command:

> help(Binomial)
These commands work just like the commands for the normal distribution. The binomial distribution requires two extra parameters, the number of trials and the probability of success for a single trial. The commands follow the same kind of naming convention, and the names of the commands are dbinom, pbinom, qbinom, and rbinom.

A few examples are given below to show how to use the different commands. First we have the distribution function, dbinom:

> x <- seq(0,50,by=1)
> y <- dbinom(x,50,0.2)
> plot(x,y)
> y <- dbinom(x,50,0.6)
> plot(x,y)
> x <- seq(0,100,by=1)
> y <- dbinom(x,100,0.6)
> plot(x,y)
Next we have the cumulative probability distribution function:

> pbinom(24,50,0.5)
[1] 0.4438624
> pbinom(25,50,0.5)
[1] 0.5561376
> pbinom(25,51,0.5)
[1] 0.5
> pbinom(26,51,0.5)
[1] 0.610116
> pbinom(25,50,0.5)
[1] 0.5561376
> pbinom(25,50,0.25)
[1] 0.999962
> pbinom(25,500,0.25)
[1] 4.955658e-33
Next we have the inverse cumulative probability distribution function:

> qbinom(0.5,51,1/2)
[1] 25
> qbinom(0.25,51,1/2)
[1] 23
> pbinom(23,51,1/2)
[1] 0.2879247
> pbinom(22,51,1/2)
[1] 0.200531
Finally random numbers can be generated according to the binomial distribution:

> rbinom(5,100,.2) 
[1] 30 23 21 19 18
> rbinom(5,100,.7)
[1] 66 66 58 68 63
> 
The Chi-Squared Distribution

There are four functions that can be used to generate the values associated with the Chi-Squared distribution. You can get a full list of them and their options using the help command:

> help(Chisquare)
These commands work just like the commands for the normal distribution. The first difference is that it is assumed that you have normalized the value so no mean can be specified. The other difference is that you have to specify the number of degrees of freedom. The commands follow the same kind of naming convention, and the names of the commands are dchisq, pchisq, qchisq, and rchisq.

A few examples are given below to show how to use the different commands. First we have the distribution function, dchisq:

> x <- seq(-20,20,by=.5)
> y <- dchisq(x,df=10)
> plot(x,y)
> y <- dchisq(x,df=12)
> plot(x,y)
Next we have the cumulative probability distribution function:

> pchisq(2,df=10)
[1] 0.003659847
> pchisq(3,df=10)
[1] 0.01857594
> 1-pchisq(3,df=10)
[1] 0.981424
> pchisq(3,df=20)
[1] 4.097501e-06
> x = c(2,4,5,6)
> pchisq(x,df=20)
[1] 1.114255e-07 4.649808e-05 2.773521e-04 1.102488e-03
Next we have the inverse cumulative probability distribution function:

> qchisq(0.05,df=10)
[1] 3.940299
> qchisq(0.95,df=10)
[1] 18.30704
> qchisq(0.05,df=20)
[1] 10.85081
> qchisq(0.95,df=20)
[1] 31.41043
> v <- c(0.005,.025,.05)
> qchisq(v,df=253)
[1] 198.8161 210.8355 217.1713
> qchisq(v,df=25)
[1] 10.51965 13.11972 14.61141
Finally random numbers can be generated according to the Chi-Squared distribution:

> rchisq(3,df=10)
[1] 16.80075 20.28412 12.39099
> rchisq(3,df=20)
[1] 17.838878  8.591936 17.486372
> rchisq(3,df=20)
[1] 11.19279 23.86907 24.81251

Continuous Probability Distributions
 
The continuous uniform distribution is commonly used in simulation.
The Normal distribution


rnorm(n=15)                         	          	             #15 random numbers, mean  = 0 , std. deviation = 1
rnorm(n=15,mean= 17)                         	             #set the mean to 17
rnorm(n=15,mean= 17,sd=4)           	             #set the standard deviation to 4
rnorm(15,17,4)                         	          	             #argument matching : default positions
 
 
other distributions
Special mathematical functions related to the beta and gamma functions.
Usage

beta(a, b)
lbeta(a, b)

gamma(x)
lgamma(x)
psigamma(x, deriv = 0)
digamma(x)
trigamma(x)

choose(n, k)
lchoose(n, k)
factorial(x)
lfactorial(x)


 
Central Limit Theorem
Hypothesis testing and con dence interval construction are based on the Central Limit Theorem.
CLT - see Introductory Data Analysis notes by Dr. Ailish Hannigan.
Can check the CLT using a small simulation example.
We will take 10000 samples of size 5 from data with a uniform distribution and record the means.
When we plot a histogram of the means, should have a normal distribution.
 
means <- numeric(10000)
for(i in 1:10000){
means[i] <- mean(runif(5))
}
hist(means)
 

Recall the Dice experiment in week 8.
 
N=100           	          	          	             #number of loops
Avgs=numeric(N)           	             #array “Avgs” store the sample means
for( i in 1:N)
              {              Dice=floor(runif(50,min=1,max=7));              Avgs[i]=mean(Dice);
              }
Avgs           	          	          	             #print Avgs dataset to screen
 
The Central limit theorem states that.

The “Dice” distribution is a discrete uniform distribution. However

mean(Avgs)           	          	             #compute the mean. Is it roughly what we are expecting?
qqnorm(Avgs)           	          	             #draws a QQ plot that is used to check for normality.
qqline(Avgs)           	          	             #adds trend line to QQplot.
shapiro.test(Avgs)           	             #Shapiro Wilk test. Normality is assumed if p-value > 0.05.
	                                     


crime=c(761,780 ,593,715,1078,567,456,686,1206,723,261 ,326,282 ,960,489,496,463,1062,805,998,126 ,792,327 ,744,434,178,679,82,339,138,627,930,875,1074,504,635,503,418,402,1023,208,766,762,301 ,372,114,515,264,208,286,2922 )

murder=c(9,11.6 ,10.2 ,8.6 ,13.1 ,5.8 ,6.3 ,5 ,8.9 ,11.4 ,3.8 ,2.3 ,2.9 ,11.4 ,7.5 ,6.4 ,6.6 ,20.3 ,3.9 ,12.7 ,1.6 ,9.8 ,3.4 ,11.3 ,13.5 ,3 ,11.3 ,
1.7 ,3.9 ,2 ,5.3 ,8 ,10.4 ,13.3 ,6 ,8.4 ,4.6 ,6.8 ,3.9 ,10.3 ,3.4 ,10.2 ,11.9 ,3.1 ,8.3 ,3.6 ,5.2 ,4.4 ,6.9 ,3.4 ,8.5 )

pctmetro=c(41.8 ,67.4 ,44.7 ,84.7 ,96.7 ,81.8 ,95.7 ,82.7 ,93 ,67.7 ,74.7 ,43.8 ,30 ,84 ,71.6 ,54.6 ,48.5 ,75 , 96.2 , 92.8 ,35.7 ,
82.7 ,69.3 ,68.3 ,30.7 ,24 ,66.3 ,41.6 ,50.6 ,59.4 ,100 ,56 ,84.8 ,91.7 ,81.3 ,60.1 ,70 ,84.8 ,93.6 ,69.8 ,32.6 ,67.7 ,83.9 ,77.5 ,77.5 ,27 ,83 ,68.1 ,41.8 ,29.7 ,100 )

pctwhite=c(75.2 ,73.5 ,82.9 ,88.6 ,79.3 ,92.5 ,89 ,79.4 ,83.5 ,70.8 ,40.9 ,96.6 ,96.7 ,81 ,90.6 ,90.9 ,91.8 ,66.7 ,91.1 ,68.9 ,98.5 ,83.1 ,94 ,87.6 ,63.3 ,92.6 ,75.2 ,94.2 ,94.3 ,98 ,80.8 ,87.1 ,86.7 ,77.2 ,87.5 ,82.5 ,93.6 ,88.7 ,92.6 ,68.6 ,90.2 ,82.8 ,85.1 ,94.8 ,77.1 ,98.4 ,89.4 ,92.1 ,96.3 ,95.9 ,31.8 )

pcths=c(86.6 ,66.9 ,66.3 ,78.7 ,76.2 ,84.4 ,79.2 ,77.5 ,74.4 ,70.9 ,80.1 ,80.1 ,79.7 ,76.2 ,75.6 ,81.3 ,64.6 ,68.3 ,80 ,78.4 ,78.8 ,76.8 ,82.4 ,73.9 ,64.3 ,81 ,70 ,76.7 ,81.8 , 82.2 ,76.7 ,75.1 ,78.8 ,74.8 ,75.7 ,74.6 ,81.5 ,74.7 ,72 ,68.3 ,77.1 ,67.1 ,72.1 ,85.1 ,75.2 ,80.8 ,83.8 ,78.6 ,66 ,83 ,73.1 )

poverty=c(9.1 ,17.4 ,20 ,15.4 ,18.2 ,9.9 ,8.5 ,10.2 ,17.8 ,13.5 ,8 ,10.3 ,13.1 ,13.6 ,12.2 ,13.1 ,20.4 ,26.4 ,10.7 ,9.7 ,10.7 ,15.4 ,11.6 ,16.1 ,24.7 ,14.9 ,14.4 ,11.2 ,10.3 ,9.9 ,10.9 ,17.4 ,9.8 ,16.4 ,13 ,19.9 ,11.8 ,13.2 ,11.2 ,18.7 ,14.2 ,19.6 ,17.4 ,10.7 ,9.7 ,10 ,12.1 ,12.6 ,22.2 ,13.3 ,26.4 )




\newpage
%------------------------------------------%




%======================================================================================================== %
1. Starting R the first time


When running R from the computer lab, your M drive will be the working directory where your work will be saved by default. You can easily change to another directory at any time by selecting Change Dir... from the File menu. 
You can save your work at any time. Select Save Workspace from the File menu. You will usually save the workspace in the working directory. When you quit R you will be prompted to save your workspace. 
%======================================================================================================= %
2. Some things to keep in mind


Everything in R is some kind of object. Objects can have different modes (numeric, character, list, function, etc.) with different structures (scalar, vector, matrix, etc.) and different classes (data frame, linear models result, etc.). 
Almost every command you execute in R uses one or more functions. Functions are called by their name followed by a set of parentheses. If any arguments are passed to the function, they are listed within the parentheses. The parentheses must always be present whether or not there are any arguments. For example, to get a listing of all the objects in your working directory, you would use objects(). If you wanted a list of objects in another directory in your search path, you might use objects(where=3). 

Use the assignment operator to create objects. The assignment operator is the "less than" symbol followed by a hyphen ( <- ). With S-PLUS you can use the underscore for the assignment operator, but that will not work with R. Apparently the equal sign can be used, but then that will not work in S-PLUS. The best practice is to use the less than and hyphen for assignments. For example, to create an object called tmp and assign it the value 3, you would enter tmp <- 3. The equal sign (=) is mainly used for passing arguments to functions, like the last command in comment b above. 

R is case sensitive. Keep that in mind when you're naming objects or calling functions. We could create another object called Tmp that would be separate and distinct from tmp. 

If you already have an object with the name tmp and you assign something else to an object with that name, then the first object is overwritten. Be careful not to lose something you want to keep. 

Once you've created objects, you may want to get rid of them later. Use the function rm() with the object names as arguments. For example, rm(tmp). 

You can recall previous commands with the up-arrow and down-arrow keys. Once you've located the command you want, you can hit enter to execute the command as is, or you can edit the line first. This can save time, especially with complicated commands. 

Open a graphics window with the function win.graph(). 

Make use of the online help. Select HTML Help from the Help menu, click on Packages, then click on Graphics and look up win.graph. You'll find a description of all possible arguments that can be used, a full discussion on its use, and some examples of how it can be used. If you just need a reminder of what arguments can be passed to a particular function, use the args() function with the function name in the parentheses. For example, try args(win.graph) to see what arguments can be used with that function and what default values they may have. Most common functions can be found in the Base package. 

In the examples that follow, pay very close attention to all associated punctuation. Things like commas and parentheses are absolutely critical to R understanding what you want to do. If you get an error after executing a command, the first thing to do is check the syntax. That is the cause of most errors. R almost always ignores spaces, so whether you type tmp<-c(1,2,3) or tmp <- c ( 1, 2, 3), you get the same result. 
The Escape key serves as your abort button. If something goes wrong or you're suddenly seeing an endless array of numbers scrolling by, you can hit the Escape key to quit whatever you're doing and get you back to the command prompt. This does not kick you out of R altogether. 
The command line interface in R uses a red font to show your input and a blue font to show results. I've tried to duplicate that in the examples in this tutorial. 

\section{The \texttt{summary()} command}
\texttt{summary()} is a generic but very useful, function to summarize many types of \texttt{R} objects, including datasets. When used on a dataset, summary returns distributional summaries of variables in the dataset.

\section{Creating Data with \texttt{R}}

\subsection{Data Import}
It is necessary to import outside data into \texttt{R} before you start analysing it. Here we will look at some relevant issues.

\subsubsection{Microsoft XLS File}
Very often, the sample data is in MS Excel$^{\mbox{\tiny{TM}}}$ format, and needs to be imported into \texttt{R} prior to use. For this, we could use the \texttt{read.xls()} function from the \textbf{\textit{gdata}} package. The command reads from an Excel spreadsheet and returns a data frame. The following shows how to load an Excel spreadsheet named ``mydata.xls". As the package is not in the core \texttt{R} library, it has to be installed and loaded into the \texttt{R} workspace. (This spreadsheet is saved in the working directory).
\begin{verbatim}
> library(gdata)                   # load the gdata package
> help(read.xls)                   # documentation
> mydata = read.xls("mydata.xls")  # read from first sheet
\end{verbatim}


\subsection{Using the \texttt{scan()} command to input character data}
Previously we have seen the \texttt{scan()} command used to quickly input numeric data. The command can also be used to input character data. The addition argument , \texttt{what=" "}, must be used.  
Create a variable “grouping” that comprises, in order,  five “A”s and then six “B”s.

\begin{verbatim}
# inputting character data

grouping = scan(what=" ")

# Enter values
# Hit return again when you have finished.

\end{verbatim}


\subsection{Spreadsheet Interface}
\texttt{R} provides a spreadsheet interface for editing the values of existing data sets.
We use the command \texttt{data.entry()} , and name of the data object as the argument.

\begin{verbatim}
> data.entry(X) # Edit the data set and exit interface
> X
\end{verbatim}


%Similarly to read.csv and read.csv2, the functions write.csv and write.csv2 are provided as wrappers to read.table, with appropriate options set to produce comma- or semicolon-separated files.

%\newpage




%We can use this approach to create a data frame :
%> data.frame(rbind(X,Y))
 % X1 X2 X3
%X  1  2  3
%Y  4  5  6

%We can then use rownames and colnames to assign meaningful names to this data frame.

%---- END OF CREATING DATA
%-------------------------------------------------------------------------------------------------------%
%----------------------------------------------------------------------------------CREATING DATA--------%

%\section{Data Entry Methods}
%\subsection{Using the \texttt{scan()} command}



%\subsection{Importing and Exporting Data}
%\subsection{The \texttt{read.csv()} command}
%\subsection{The \texttt{write.csv()} command}
%\subsection{The \texttt{sink()} command}

%-------------------------------------------------------------------------%
%\subsection{Classes of Data Objects}
%\begin{verbatim}
%class(Numvec)
%class(Charvec)
%class(A)
%class(iris)
%class(Nile)
%\end{verbatim}



%----------------------------------------------------------------------------------------------------%
\section{Indexing and Subsetting }
\subsection{Relational and Logical Operators}

Relational operators allow for the comparison of values in vectors.
\begin{center}
\begin{tabular}{|c|c|}
  \hline
greater than &	$>$\\
less than&	$<$\\
equal to	&$==$\\
less than or equal to&	$<=$\\
greater than or equal to&	$>=$\\
not equal to	&$!=$\\
  \hline
\end{tabular}
\end{center}


Note the difference of the equality operator "==" with assignment operator "=".

\& and \&\& indicate logical AND %and $\|$ and $\|\|$ indicate logical OR.
The shorter form performs element-wise comparisons in much the same way as arithmetic operators. The longer form is appropriate for programming control-flow and typically preferred in "if" clauses.
\begin{itemize}
\item We can use relational operators to subset vectors (as well as more complex data objects such as data frames, which we will meet later).
\item We specify the  relational condition in square brackets.
\item We can construct compound relational conditions too, using logical operators
\end{itemize}
%----------------------------------%
\begin{framed}
\begin{verbatim}
> vec=1:19
> vec[vec<5]
[1] 1 2 3 4
> vec[(vec<6)|(vec>16)]
[1]  1  2  3  4  5 17 18 19
\end{verbatim}
\end{framed}

\subsection{Conditional Subsetting}
 \texttt{The Subset command}
%------------------------------------%
\subsection{Selection using the Subset Function}
The subset( ) function is the easiest way to select variables and observeration. In the following example, we select all rows that have a value of age greater than or equal to 20 or age less then 10. We keep the ID and Weight columns.

% End of relational and Logical Operators




\section{A Brief Introduction to fitting Linear Models (Lists)}

A very commonly used statistical procedure is \textbf{simple linear regression}
\begin{itemize}
\item \texttt{lm()}
\item \texttt{summary()}
\end{itemize}

\begin{framed}
\begin{verbatim}
Y <- c( )
X <- c( )

plot(X,Y)
cor(X,Y)
lm(Y~X)
\end{verbatim}
\end{framed}
%------------------------%
\begin{framed}
\begin{verbatim}
FitA =lm(Y~X)
summary(FitA)
\end{verbatim}
\end{framed}
%--------------------------------%
Let's look at this summary output in more detail, to see how it is structured. Importantly this object is structured as a list of named components.
\begin{framed}
\begin{verbatim}
names(summary(FitA))
class(summary(FitA))
mode(summary(FitA))
str(summary(FitA))
\end{verbatim}
\end{framed}

%-------------------------------%
The summary of \texttt{FitA} is a data object in it's own right. We will save it under the name \texttt{Sum.FitA} (N.B. The dot in the name has no particular meaning).
\begin{framed}
\begin{verbatim}
Sum.FitA=summary(FitA)
Sum.FitA[1]
Sum.FitA$pvalue
\end{verbatim}
\end{framed}
%------------------------------%
Suppose we wish require the $p-$value for the slope estimate only.
\begin{framed}
\begin{verbatim}
class(Sum.FitA$pvalue)
mode(Sum.FitA$pvalue)
dim(Sum.FitA$pvalue)
\end{verbatim}
\end{framed}
%-------------------------------------------------%
%--------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------APPLY FAMILY--%
\section{The \texttt{apply()} family of functions}

The "apply" family of functions keep you from having to write loops to perform 
some operation on every row or every column of a matrix or data frame, or on 
every element in a list.

\subsection{The \texttt{apply()} function}
The \texttt{apply()} function is a powerful device that operates on arrays and,
 in particular, matrices.
The \texttt{apply()} function returns a vector (or array or list of values) 
obtained by applying a specified function to either the row or columns of 
an array or matrix.
To specify use for rows or columns, use the additional argument of 1 for rows, 
and 2 for columns.
\begin{framed}
\begin{verbatim}
# create a matrix of 10 rows x 2 columns
m <- matrix(c(1:10, 11:20), nrow = 10, ncol = 2)

# mean of the rows

apply(m, 1, mean)
# [1]  6  7  8  9 10 11 12 13 14 15

# mean of the columns
apply(m, 2, mean)
#[1]  5.5 15.5
\end{verbatim}
\end{framed}

The local version of \texttt{apply()} is \texttt{lapply()}, which computes a function for each 
argument of a list, provided each argument is compatible with the function argument (e.g. that is numeric).

The \texttt{lapply()} command returns a list of the same length as a list \texttt{X}, each 
element of which is the result of applying a specified function to 
the corresponding element of X.

\subsubsection{The \texttt{sapply()} command}
A user friendly version of  \texttt{lapply()}  is  \texttt{sapply()} .The \texttt{sapply()} command  is a variant of \texttt{lapply()} , returning a matrix 
instead of a list - again of the same length as a list X, 
each element of which is the result of applying a specified function to the
 corresponding element of X.
\begin{verbatim}
> x <- list(a=1:10, b=exp(-3:3), logic=c(T,F,F,T))
>
> # compute the list mean for each list element
>
> lapply(x,mean)
$a
[1] 5.5

$b
[1] 4.535125

$logic
[1] 0.5
>
> sapply(x,mean)
       a        b    logic
5.500000 4.535125 0.500000
>
\end{verbatim}





%-------------------------------------------------------------------------------------------------------------%

R is an open-source statistical package based on the S language. It is a powerful computing tool that combines the usefulness of a statistical analysis package with that of a publication quality graphics package and a matrix-based programming language. It's easy enough to use for quick and simple tasks, yet powerful enough for the most demanding ones. The goal of this demonstration is to provide a basic introduction to using R. An R session differs from that of other statistical software. You will find it to be an interactive approach where the results from one step lead to the next. This introduction to R is necessarily limited in scope to only a handful of analyses. Once you become familiar with R and browse through some of the online help topics, you will discover tools for practically any type of analysis you need. S-PLUS is a commercial application also based on the S language. Much of R is identical to the command line useage of S-PLUS. There are differences though in some functions and their arguments so existing S-PLUS code may require some modification to run in R. 
%================================================================================================== %
Topics included in this tutorial: 
1. Starting R the first time
2. Some things to keep in mind
3. Beginning an analysis
4. Visualizing your data
%======================================================================================================== %
3. Beginning an analysis


For the remainder of this tutorial, we will be analyzing the following dataset: 
Concentration
0.3330
0.1670
0.0833
0.0416
0.0208
0.0104
0.0052
Velocity
3.636
3.636
3.236
2.666
2.114
1.466
0.866
These data are measurements of the rate or velocity of a chemical reaction for different concentrations of substrate. We are interested in fitting a model of the relationship between concentration and velocity. The first thing we need to do is enter the data. We can do this from the Commands Window, from a spreadsheet interface, or by importing an existing file. Most of this tutorial will focus on command line input so let's begin there. At the prompt, create two vectors: 
> conc <- c(0.3330, 0.1670, 0.0833, 0.0416, 0.0208, 0.0104, 0.0052)
> vel <- c(3.636, 3.636, 3.236, 2.660, 2.114, 1.466, 0.866)
We use the function c(), which stands for concatenate, to create a vector. The individual elements can be numeric, as in this example, or character, or any other mode. However, all elements in a vector must be the same mode. Note that the elements are separated by commas. The spaces between elements are included for clarity and are not required by R. 
Now, we will combine these two vectors into a single data frame: 
> df <- data.frame(conc, vel)
Let's look at the data frame to be sure we entered the data correctly. To view any object in R, simply type its name: 
> df
conc   vel
1 0.3330 3.636
2 0.1670 3.636
3 0.0833 3.236
4 0.0416 2.660
5 0.0208 2.114
6 0.0104 1.466
7 0.0052 0.866
Oops. It looks like there is an error in one of the velocity entries. The fourth one down, 2.660, should be 2.666. You could use the up-arrow to recall the command you used to create vel, make the change, and run it again. Then use the up-arrow again to recall the command you used to create df and run that one again. Let's look at another way to do it. First, we'll change just that one element in the vector vel. Individual elements in a vector are referenced in square brackets. We want to change the fourth element, so we type: 
> vel[4] <- 2.666
Take a look at vel to see that it has been changed. There are a couple ways to change elements in a data frame. Treating the data frame as a matrix, we can reference the element in the fourth row and second column like this: 
> df[4,2] <- 2.666
Note the order that the row and column are referenced: first the row, then the column. Data frames also allow us to reference individual columns by their names. This is done with the name of the data frame, followed by a dollar sign, and the name of the column. So the vel data can be referenced as df$vel. Now we can change the fourth element like this: 
> df$vel[4] <- 2.666
Note that df$vel is a vector so we only need one number in the brackets. Let's take another look at df to be sure we have it right this time. 
\begin{framed}
\begin{verbatim}
> df
conc   vel 
1 0.3330 3.636
2 0.1670 3.636
3 0.0833 3.236
4 0.0416 2.666
5 0.0208 2.114
6 0.0104 1.466
7 0.0052 0.866
\end{verbatim}
\end{framed}
Looks good! We could have created this data frame in other ways. If the data were already entered into a spreadsheet, database, or ASCII file, you could import it using the appropriate commands. Alternatively, you could create a data frame using a spreadsheet-like interface right in R. From the Edit choose Data Editor... and follow the prompts. You can use the fix() command to edit existing data frames. 
Now we're ready to do an analysis.

%======================================================================================================== %
\subsection{4. Visualizing your data}


The first thing we want to do is plot the data. You can easily get a basic plot with the following:

> plot(df$conc, df$vel)

The first vector given is the independent variable to be plotted on the X-axis, the second is the dependent variable for the Y-axis. If you execute a plotting function and there is no active graphsheet, a default graphsheet will be opened for you. If you then do another plot, the first one will be lost and the new plot will be drawn in the 
same graphics window. You can keep your first plot by opening another graphsheet by typing: 

> win.graph()

The default is a square graphsheet. You can create portrait graphsheets, landscape graphsheets, any shape you want. Check win.graph in the help files to see how. 
We're going to look at 3 different ways to analyze these data: simple linear regression, non-linear regression, and polynomial regression. 


%------------------------------------------------------------------------------------------------------%
\newpage
Using the R environment
Views R Console
Graphics Console
Script Editor Console
passing a script for compiling

Creating and Editting a script using script editor

Using R Studio Integrated Development Environment

Changing GUI options

length
ls()
getwd
setwd
head()
length()
rbind()
cbind()

basic R editor
 - new script
 - updating script
 - running script
 -
"summary" is a generic function used to produce result summaries of the results of various model fitting functions. 
The function invokes particular methods which depend on the class of the first argument. 
  - attach()
  - data()
   - Loads specified data sets, or list the available data sets. 
  - ?
  - data.entry
  - c()
- Combine Values into a Vector or List
  - assignment operator
  - Sys.time and Sys.Date returns the system's idea of the current date with and without time. 
  - q()


mode() - storage mode of a data object( data structure)

"summary" is a generic function used to produce result summaries of the results of various model fitting functions. 
The function invokes particular methods which depend on the class of the first argument. 
  - attach()
  - data()
   - Loads specified data sets, or list the available data sets. 
  - ?
  - data.entry
  - c()
- Combine Values into a Vector or List
  - assignment operator
  - Sys.time and Sys.Date returns the system's idea of the current date with and without time. 
  - q()


mode() - storage mode of a data object( data structure)





Saving your workspace
Listing objects in the workspace
Removing objects from the workspace
quitting the environment

data entry
Spreadsheet Style Interface
Vectors
Lists
Output from many important statistical functions are structured as lists.
Arrays
Matrices important in advanced computational disciplines like Machine Learning.

 
The summary function


Lists
A list in R is a rather loose objects made of a collection of other arbitary objects known as its components. 








Factors

a factor is a vector of characters or integers that are used to specify a discrete classification of the component of vectors.

logical subscripting

\end{document}
